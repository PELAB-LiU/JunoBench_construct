{
  "executed": [
    {
      "execution_count": 1,
      "code_cell_id": 0,
      "code": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sn\nimport skimage.io\nimport keras.backend as K\nimport tensorflow as tf\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau,ModelCheckpoint,EarlyStopping\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.layers import Dense, Input, Dropout, Flatten, Conv2D"
    },
    {
      "execution_count": 2,
      "code_cell_id": 1,
      "code": "train_datagen = ImageDataGenerator(\n width_shift_range = 0.1,\n height_shift_range = 0.1,\n horizontal_flip = True,\n rescale = 1./255,\n\n validation_split = 0.2\n )\nvalid_datagen = ImageDataGenerator(rescale = 1./255,\n validation_split = 0.2)\ntest_datagen = ImageDataGenerator(rescale = 1./255,\n validation_split = 0.2)"
    },
    {
      "execution_count": 3,
      "code_cell_id": 2,
      "code": "train_dataset=train_datagen.flow_from_directory(directory='data_small/train',\n target_size=(48,48),\n class_mode='categorical',\n subset='training',\n batch_size=64)"
    },
    {
      "execution_count": 4,
      "code_cell_id": 3,
      "code": "valid_dataset=valid_datagen.flow_from_directory(directory='data_small/test',\n target_size=(48,48),\n class_mode='categorical',\n batch_size=64)"
    },
    {
      "execution_count": 5,
      "code_cell_id": 4,
      "code": "test_dataset=test_datagen.flow_from_directory(directory='data_small/test',\n target_size=(48,48),\n class_mode='categorical',\n batch_size=64)"
    },
    {
      "execution_count": 6,
      "code_cell_id": 5,
      "code": "train_datagen = ImageDataGenerator(\n\n rotation_range=20,\n zoom_range=0.2\n)"
    },
    {
      "execution_count": 7,
      "code_cell_id": 7,
      "code": "from tensorflow.keras.layers import BatchNormalization, Activation, MaxPooling2D\nfrom tensorflow.keras.layers import SeparableConv2D\nfrom tensorflow.keras.layers import concatenate\nfrom tensorflow.keras.layers import GlobalAveragePooling2D\nfrom tensorflow.keras import Model\nfrom tensorflow.keras.layers import LeakyReLU\nfrom tensorflow.keras.regularizers import l2\n\ninputs=Input((64,64,3))\n\nh=Conv2D(64,(1,1),padding='same',activation='relu')(inputs)\nh=BatchNormalization()(h)\nh = Activation('relu')(h)\nh=Conv2D(64,(3,3))(h)\nh=BatchNormalization()(h)\n\nh=Activation('relu')(h)\n\nb=Conv2D(128,(1,1),strides=(2,2))(h)\nb=BatchNormalization()(b)\n\nh=SeparableConv2D(128,(3,3),padding='same')(h)\nh=BatchNormalization()(h)\nh = Activation('relu')(h)\nh=Activation('relu')(h)\nh=SeparableConv2D(128,(3,3),padding='same')(h)\nh=BatchNormalization()(h)\nh=MaxPooling2D((2,2),strides=(2,2))(h)\n\nh=concatenate([h,b],name='first')\n\nb=Conv2D(128,(2,2),strides=(2,2))(h)\nb=BatchNormalization()(b)\n\nh=SeparableConv2D(128,(3,3),padding='same')(h)\nh=BatchNormalization()(h)\nh = Activation('relu')(h)\nh=SeparableConv2D(128,(3,3),padding='same')(h)\nh=BatchNormalization()(h)\nh=MaxPooling2D((2,2),strides=(2,2))(h)\n\nh=concatenate([h,b],name='second')\n\nb=Conv2D(256,(1,1),padding='same')(h)\nb=BatchNormalization()(b)\nb=MaxPooling2D((2,2),strides=(2,2))(b)\n\nh=SeparableConv2D(256,(3,3),padding='same')(h)\nh=BatchNormalization()(h)\n\nh=Activation('relu')(h)\nh=SeparableConv2D(256,(3,3),padding='same')(h)\nh=BatchNormalization()(h)\nh=MaxPooling2D((2,2),strides=(2,2))(h)\n\nh=concatenate([h,b],name='third')\nb=h\n\nh = Activation('relu')(h)\nh=SeparableConv2D(512,(3,3),padding='same')(h)\nh=BatchNormalization()(h)\nh = Activation('relu')(h)\nh=SeparableConv2D(512,(3,3),padding='same')(h)\nh=BatchNormalization()(h)\nh = Activation('relu')(h)\nh=SeparableConv2D(512,(3,3),padding='same')(h)\nh=BatchNormalization()(h)\n\nh=concatenate([h,b],name='fourth')\n\nb=Conv2D(512,(1,1),padding='same')(h)\nb=BatchNormalization()(b)\nb=MaxPooling2D((2,2),strides=(2,2))(b)\n\nh=SeparableConv2D(512,(3,3),padding='same')(h)\nh=BatchNormalization()(h)\nh = Activation('relu')(h)\nh=SeparableConv2D(512,(3,3),padding='same')(h)\nh=BatchNormalization()(h)\nh=MaxPooling2D((2,2),strides=(2,2))(h)\n\nh=concatenate([h,b],name='fifth')\n\nb=Conv2D(1024,(1,1),padding='same')(h)\nb=BatchNormalization()(b)\nb=MaxPooling2D((2,2),strides=(2,2))(b)\n\nh=SeparableConv2D(1024,(3,3),padding='same')(h)\nh = Activation('relu')(h)\nh=SeparableConv2D(1024,(3,3),padding='same')(h)\nh=BatchNormalization()(h)\nh=MaxPooling2D((2,2),strides=(2,2))(h)\n\nh=concatenate([h,b],name='sixth')\nb=h\n\nh = Activation('relu')(h)\n\nh=SeparableConv2D(512,(3,3),padding='same')(h)\nh=BatchNormalization()(h)\nh = Activation('relu')(h)\nh=SeparableConv2D(256,(3,3),padding='same')(h)\nh=BatchNormalization()(h)\nh = Activation('relu')(h)\nh=SeparableConv2D(128,(3,3),padding='same')(h)\nh=BatchNormalization()(h)\n\nh=concatenate([h,b],name='seventh')\nb=h\n\nb=Conv2D(256,(1,1),strides=(1,1))(h)\nb=BatchNormalization()(b)\n\nh = Activation('relu')(h)\nh=SeparableConv2D(1024,(3,3),padding='same')(h)\nh=BatchNormalization()(h)\nh = Activation('relu')(h)\nh=SeparableConv2D(512,(3,3),padding='same')(h)\nh=BatchNormalization()(h)\n\nh=concatenate([h,b],name='eighth')\n\nh=SeparableConv2D(256,(3,3),padding='same')(h)\nh=BatchNormalization()(h)\nh = Activation('relu')(h)\n\nx = GlobalAveragePooling2D()(h)\n\nx = Dense(1024)(x)\nx = BatchNormalization()(x)\nx = LeakyReLU(alpha=0.1)(x)\nx = Dropout(0.4)(x)\n\nx = Dense(512)(x)\nx = BatchNormalization()(x)\nx = LeakyReLU(alpha=0.1)(x)\nx = Dropout(0.4)(x)\n\nx = Dense(256)(x)\nx = BatchNormalization()(x)\nx = LeakyReLU(alpha=0.1)(x)\nx = Dropout(0.3)(x)\n\nx = Dense(128)(x)\nx = BatchNormalization()(x)\nx = LeakyReLU(alpha=0.1)(x)\nx = Dropout(0.2)(x)\n\noutputs = Dense(7, activation='softmax')(x)\n\nmodel = Model(inputs=inputs, outputs=outputs)\nmodel.summary()"
    },
    {
      "execution_count": 8,
      "code_cell_id": 10,
      "code": "def f1_score(y_true,y_pred):\n true_positives=K.sum(K.round(K.clip(y_true*y_pred,0,1)))\n possible_positives=K.sum(K.round(K.clip(y_true,0,1)))\n predicted_positives=K.sum(K.round(K.clip(y_pred,0,1)))\n precision=true_positives/(predicted_positives+K.epsilon())\n recall=true_positives/(possible_positives+K.epsilon())\n f1_val=2*(precision*recall)/(precision+recall+K.epsilon())\n return f1_val"
    },
    {
      "execution_count": 9,
      "code_cell_id": 11,
      "code": "METRICS=[\n tf.keras.metrics.BinaryAccuracy(name='accuracy'),\n tf.keras.metrics.Precision(name='precision'),\n tf.keras.metrics.Recall(name='recall'),\n tf.keras.metrics.AUC(name='auc'),\n f1_score,\n]"
    }
  ],
  "target": {
    "code_cell_id": 8,
    "code": "from sklearn.utils.class_weight import compute_class_weight\nclass_weights = compute_class_weight('balanced', np.unique(train_dataset.labels), train_dataset.labels)\nclass_weight_dict = {i: class_weights[i] for i in range(len(class_weights))}\n\nmodel.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=METRICS, class_weight=class_weight_dict)"
  }
}