{
  "executed": [
    {
      "execution_count": 1,
      "code_cell_id": 0,
      "code": "import numpy as np\n\ndef softmax(x):\n return np.exp(x) / np.sum(np.exp(x), axis=0)\n\ndef cross_entropy(x):\n return -np.log(x)\n\ndef regularized_cross_entropy(layers, lam, x):\n loss = cross_entropy(x)\n for layer in layers:\n loss += lam * (np.linalg.norm(layer.get_weights()) ** 2)\n return loss\n\ndef leakyReLU(x, alpha=0.001):\n return x * alpha if x < 0 else x\n\ndef leakyReLU_derivative(x, alpha=0.01):\n return alpha if x < 0 else 1\n\ndef lr_schedule(learning_rate, iteration):\n if iteration == 0:\n return learning_rate\n if (iteration >= 0) and (iteration <= 10000):\n return learning_rate\n if iteration > 10000:\n return learning_rate * 0.1\n if iteration > 30000:\n return learning_rate * 0.1\n\nclass Convolutional:\n\n def __init__(self, name, num_filters=16, stride=1, size=3, activation=None):\n self.name = name\n self.filters = np.random.randn(num_filters, 3, 3) * 0.1\n self.stride = stride\n self.size = size\n self.activation = activation\n self.last_input = None\n self.leakyReLU = np.vectorize(leakyReLU)\n self.leakyReLU_derivative = np.vectorize(leakyReLU_derivative)\n\n def forward(self, image):\n self.last_input = image\n\n input_dimension = image.shape[1]\n output_dimension = int((input_dimension - self.size) / self.stride) + 1\n\n out = np.zeros((self.filters.shape[0], output_dimension, output_dimension))\n\n for f in range(self.filters.shape[0]):\n tmp_y = out_y = 0\n while tmp_y + self.size <= input_dimension:\n tmp_x = out_x = 0\n while tmp_x + self.size <= input_dimension:\n patch = image[:, tmp_y:tmp_y + self.size, tmp_x:tmp_x + self.size]\n out[f, out_y, out_x] += np.sum(self.filters[f] * patch)\n tmp_x += self.stride\n out_x += 1\n tmp_y += self.stride\n out_y += 1\n if self.activation == 'relu':\n self.leakyReLU(out)\n return out\n\n def backward(self, din, learn_rate=0.005):\n input_dimension = self.last_input.shape[1]\n\n if self.activation == 'relu':\n self.leakyReLU_derivative(din)\n\n dout = np.zeros(self.last_input.shape)\n dfilt = np.zeros(self.filters.shape)\n\n for f in range(self.filters.shape[0]):\n tmp_y = out_y = 0\n while tmp_y + self.size <= input_dimension:\n tmp_x = out_x = 0\n while tmp_x + self.size <= input_dimension:\n patch = self.last_input[:, tmp_y:tmp_y + self.size, tmp_x:tmp_x + self.size]\n dfilt[f] += np.sum(din[f, out_y, out_x] * patch, axis=0)\n dout[:, tmp_y:tmp_y + self.size, tmp_x:tmp_x + self.size] += din[f, out_y, out_x] * self.filters[f]\n tmp_x += self.stride\n out_x += 1\n tmp_y += self.stride\n out_y += 1\n self.filters -= learn_rate * dfilt\n return dout\n\n def get_weights(self):\n return np.reshape(self.filters, -1)\n\nclass Pooling:\n def __init__(self, name, stride=2, size=2):\n self.name = name\n self.last_input = None\n self.stride = stride\n self.size = size\n\n def forward(self, image):\n self.last_input = image\n\n num_channels, h_prev, w_prev = image.shape\n h = int((h_prev - self.size) / self.stride) + 1\n w = int((w_prev - self.size) / self.stride) + 1\n\n downsampled = np.zeros((num_channels, h, w))\n\n for i in range(num_channels):\n curr_y = out_y = 0\n while curr_y + self.size <= h_prev:\n curr_x = out_x = 0\n while curr_x + self.size <= w_prev:\n patch = image[i, curr_y:curr_y + self.size, curr_x:curr_x + self.size]\n downsampled[i, out_y, out_x] = np.max(patch)\n curr_x += self.stride\n out_x += 1\n curr_y += self.stride\n out_y += 1\n\n return downsampled\n\n def backward(self, din, learning_rate):\n num_channels, orig_dim, *_ = self.last_input.shape\n\n dout = np.zeros(self.last_input.shape)\n\n for c in range(num_channels):\n tmp_y = out_y = 0\n while tmp_y + self.size <= orig_dim:\n tmp_x = out_x = 0\n while tmp_x + self.size <= orig_dim:\n patch = self.last_input[c, tmp_y:tmp_y + self.size, tmp_x:tmp_x + self.size]\n (x, y) = np.unravel_index(np.nanargmax(patch), patch.shape)\n dout[c, tmp_y + x, tmp_x + y] += din[c, out_y, out_x]\n tmp_x += self.stride\n out_x += 1\n tmp_y += self.stride\n out_y += 1\n\n return dout\n\n def get_weights(self):\n return 0\n\nclass FullyConnected:\n def __init__(self, name, nodes1, nodes2, activation):\n self.name = name\n self.weights = np.random.randn(nodes1, nodes2) * 0.1\n self.biases = np.zeros(nodes2)\n self.activation = activation\n self.last_input_shape = None\n self.last_input = None\n self.last_output = None\n self.leakyReLU = np.vectorize(leakyReLU)\n self.leakyReLU_derivative = np.vectorize(leakyReLU_derivative)\n\n def forward(self, input):\n self.last_input_shape = input.shape\n\n input = input.flatten()\n\n output = np.dot(input, self.weights) + self.biases\n\n if self.activation == 'relu':\n self.leakyReLU(output)\n\n self.last_input = input\n self.last_output = output\n\n return output\n\n def backward(self, din, learning_rate=0.005):\n if self.activation == 'relu':\n self.leakyReLU_derivative(din)\n\n self.last_input = np.expand_dims(self.last_input, axis=1)\n din = np.expand_dims(din, axis=1)\n\n dw = np.dot(self.last_input, np.transpose(din))\n db = np.sum(din, axis=1).reshape(self.biases.shape)\n\n self.weights -= learning_rate * dw\n self.biases -= learning_rate * db\n\n dout = np.dot(self.weights, din)\n return dout.reshape(self.last_input_shape)\n\n def get_weights(self):\n return np.reshape(self.weights, -1)\n\nclass Dense:\n def __init__(self, name, nodes, num_classes):\n self.name = name\n self.weights = np.random.randn(nodes, num_classes) * 0.1\n self.biases = np.zeros(num_classes)\n self.last_input_shape = None\n self.last_input = None\n self.last_output = None\n\n def forward(self, input):\n self.last_input_shape = input.shape\n\n input = input.flatten()\n\n output = np.dot(input, self.weights) + self.biases\n\n self.last_input = input\n self.last_output = output\n\n return softmax(output)\n\n def backward(self, din, learning_rate=0.005):\n for i, gradient in enumerate(din):\n if gradient == 0:\n continue\n\n t_exp = np.exp(self.last_output)\n dout_dt = -t_exp[i] * t_exp / (np.sum(t_exp) ** 2)\n dout_dt[i] = t_exp[i] * (np.sum(t_exp) - t_exp[i]) / (np.sum(t_exp) ** 2)\n\n dt = gradient * dout_dt\n\n dout = self.weights @ dt\n\n self.weights -= learning_rate * (np.transpose(self.last_input[np.newaxis]) @ dt[np.newaxis])\n self.biases -= learning_rate * dt\n\n return dout.reshape(self.last_input_shape)\n\n def get_weights(self):\n return np.reshape(self.weights, -1)"
    },
    {
      "execution_count": 2,
      "code_cell_id": 1,
      "code": "!pip install --no-deps idx2numpy"
    },
    {
      "execution_count": 3,
      "code_cell_id": 2,
      "code": "import tensorflow as tf\nimport os\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport idx2numpy\nimport numpy as np\nfrom six.moves import cPickle\nimport platform\nimport cv2\nsns.set(color_codes=True)\n\ntf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n\ndef load_mnist():\n X_train = idx2numpy.convert_from_file('MNIST_data/train-images-idx3-ubyte')\n train_labels = idx2numpy.convert_from_file('MNIST_data/train-labels-idx1-ubyte')\n X_test = idx2numpy.convert_from_file('MNIST_data/t10k-images-idx3-ubyte')\n test_labels = idx2numpy.convert_from_file('MNIST_data/t10k-labels-idx1-ubyte')\n\n train_images = []\n for i in range(X_train.shape[0]):\n train_images.append(np.expand_dims(X_train[i], axis=0))\n train_images = np.array(train_images)\n\n test_images = []\n for i in range(X_test.shape[0]):\n test_images.append(np.expand_dims(X_test[i], axis=0))\n test_images = np.array(test_images)\n\n indices = np.random.permutation(train_images.shape[0])\n training_idx, validation_idx = indices[:55000], indices[55000:]\n train_images, validation_images = train_images[training_idx, :], train_images[validation_idx, :]\n train_labels, validation_labels = train_labels[training_idx], train_labels[validation_idx]\n\n return {\n 'train_images': train_images,\n 'train_labels': train_labels,\n 'validation_images': validation_images,\n 'validation_labels': validation_labels,\n 'test_images': test_images,\n 'test_labels': test_labels\n }\n\ndef load_pickle(f):\n version = platform.python_version_tuple()\n if version[0] == '2':\n return cPickle.load(f)\n elif version[0] == '3':\n return cPickle.load(f, encoding='latin1')\n raise ValueError(\"invalid python version: {}\".format(version))\n\nfrom keras.datasets import cifar10\nimport numpy as np\n\ndef load_cifar():\n\n (X_train, y_train), (X_test, y_test) = cifar10.load_data()\n\n indices = np.random.permutation(X_train.shape[0])\n training_idx, validation_idx = indices[:49000], indices[49000:]\n X_train, X_val = X_train[training_idx, :], X_train[validation_idx, :]\n y_train, y_val = y_train[training_idx], y_train[validation_idx]\n\n return {\n 'train_images': X_train,\n 'train_labels': y_train,\n 'validation_images': X_val,\n 'validation_labels': y_val,\n 'test_images': X_test,\n 'test_labels': y_test\n }\n\ndef minmax_normalize(x):\n min_val = np.min(x)\n max_val = np.max(x)\n x = (x - min_val) / (max_val - min_val)\n return x\n\ndef preprocess(dataset):\n dataset['train_images'] = np.array([minmax_normalize(x) for x in dataset['train_images']])\n dataset['validation_images'] = np.array([minmax_normalize(x) for x in dataset['validation_images']])\n dataset['test_images'] = np.array([minmax_normalize(x) for x in dataset['test_images']])\n return dataset\n\ndef plot_accuracy_curve(accuracy_history, val_accuracy_history):\n plt.plot(accuracy_history, 'b', linewidth=3.0, label='Training accuracy')\n plt.plot(val_accuracy_history, 'r', linewidth=3.0, label='Validation accuracy')\n plt.xlabel('Iteration', fontsize=16)\n plt.ylabel('Accuracy rate', fontsize=16)\n plt.legend()\n plt.title('Training Accuracy', fontsize=16)\n plt.savefig('training_accuracy.png')\n plt.show()\n\ndef plot_learning_curve(loss_history):\n plt.plot(loss_history, 'b', linewidth=3.0, label='Cross entropy')\n plt.xlabel('Iteration', fontsize=16)\n plt.ylabel('Loss', fontsize=16)\n plt.legend()\n plt.title('Learning Curve', fontsize=16)\n plt.savefig('learning_curve.png')\n plt.show()\n\ndef plot_sample(image, true_label, predicted_label):\n plt.imshow(image)\n if true_label and predicted_label is not None:\n if type(true_label) == 'int':\n plt.title('True label: %d, Predicted Label: %d' % (true_label, predicted_label))\n else:\n plt.title('True label: %s, Predicted Label: %s' % (true_label, predicted_label))\n plt.show()\n\ndef plot_histogram(layer_name, layer_weights):\n plt.hist(layer_weights)\n plt.title('Histogram of ' + str(layer_name))\n plt.xlabel('Value')\n plt.ylabel('Number')\n plt.show()\n\ndef to_gray(image_name):\n image = cv2.imread(image_name + '.png')\n image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n cv2.imshow('Gray image', image)\n cv2.imwrite(image_name + '.png', image)\n cv2.waitKey(0)\n cv2.destroyAllWindows()"
    },
    {
      "execution_count": 4,
      "code_cell_id": 3,
      "code": "import numpy as np\nimport time\n\nclass Network:\n def __init__(self):\n self.layers = []\n\n def add_layer(self, layer):\n self.layers.append(layer)\n\n def build_model(self, dataset_name):\n if dataset_name == 'mnist':\n self.add_layer(Convolutional(name='conv1', num_filters=8, stride=2, size=3, activation='relu'))\n self.add_layer(Convolutional(name='conv2', num_filters=8, stride=2, size=3, activation='relu'))\n self.add_layer(Dense(name='dense', nodes=8 * 6 * 6, num_classes=10))\n else:\n self.add_layer(Convolutional(name='conv1', num_filters=32, stride=1, size=3, activation='relu'))\n self.add_layer(Convolutional(name='conv2', num_filters=32, stride=1, size=3, activation='relu'))\n self.add_layer(Pooling(name='pool1', stride=2, size=2))\n self.add_layer(Convolutional(name='conv3', num_filters=64, stride=1, size=3, activation='relu'))\n self.add_layer(Convolutional(name='conv4', num_filters=64, stride=1, size=3, activation='relu'))\n self.add_layer(Pooling(name='pool2', stride=2, size=2))\n self.add_layer(FullyConnected(name='fullyconnected', nodes1=64 * 5 * 5, nodes2=256, activation='relu'))\n self.add_layer(Dense(name='dense', nodes=256, num_classes=10))\n\n def forward(self, image, plot_feature_maps):\n for layer in self.layers:\n if plot_feature_maps:\n image = (image * 255)[0, :, :]\n plot_sample(image, None, None)\n image = layer.forward(image)\n return image\n\n def backward(self, gradient, learning_rate):\n for layer in reversed(self.layers):\n gradient = layer.backward(gradient, learning_rate)\n\n def train(self, dataset, num_epochs, learning_rate, validate, regularization, plot_weights, verbose):\n history = {'loss': [], 'accuracy': [], 'val_loss': [], 'val_accuracy': []}\n for epoch in range(1, num_epochs + 1):\n print('\\n--- Epoch {0} ---'.format(epoch))\n loss, tmp_loss, num_corr = 0, 0, 0\n initial_time = time.time()\n for i in range(len(dataset['train_images'])):\n if i % 100 == 99:\n accuracy = (num_corr / (i + 1)) * 100\n loss = tmp_loss / (i + 1)\n\n history['loss'].append(loss)\n history['accuracy'].append(accuracy)\n\n if validate:\n indices = np.random.permutation(dataset['validation_images'].shape[0])\n val_loss, val_accuracy = self.evaluate(\n dataset['validation_images'][indices, :],\n dataset['validation_labels'][indices],\n regularization,\n plot_correct=0,\n plot_missclassified=0,\n plot_feature_maps=0,\n verbose=0\n )\n history['val_loss'].append(val_loss)\n history['val_accuracy'].append(val_accuracy)\n\n if verbose:\n print('[Step %05d]: Loss %02.3f | Accuracy: %02.3f | Time: %02.2f seconds | '\n 'Validation Loss %02.3f | Validation Accuracy: %02.3f' %\n (i + 1, loss, accuracy, time.time() - initial_time, val_loss, val_accuracy))\n elif verbose:\n print('[Step %05d]: Loss %02.3f | Accuracy: %02.3f | Time: %02.2f seconds' %\n (i + 1, loss, accuracy, time.time() - initial_time))\n\n initial_time = time.time()\n\n image = dataset['train_images'][i]\n label = dataset['train_labels'][i]\n\n tmp_output = self.forward(image, plot_feature_maps=0)\n\n tmp_loss += regularized_cross_entropy(self.layers, regularization, tmp_output[label])\n\n if np.argmax(tmp_output) == label:\n num_corr += 1\n\n gradient = np.zeros(10)\n gradient[label] = -1 / tmp_output[label] + np.sum(\n [2 * regularization * np.sum(np.absolute(layer.get_weights())) for layer in self.layers])\n\n learning_rate = lr_schedule(learning_rate, iteration=i)\n\n self.backward(gradient, learning_rate)\n\n if verbose:\n print('Train Loss: %02.3f' % (history['loss'][-1]))\n print('Train Accuracy: %02.3f' % (history['accuracy'][-1]))\n plot_learning_curve(history['loss'])\n plot_accuracy_curve(history['accuracy'], history['val_accuracy'])\n\n if plot_weights:\n for layer in self.layers:\n if 'pool' not in layer.name:\n plot_histogram(layer.name, layer.get_weights())\n\n def evaluate(self, X, y, regularization, plot_correct, plot_missclassified, plot_feature_maps, verbose):\n loss, num_correct = 0, 0\n for i in range(len(X)):\n tmp_output = self.forward(X[i], plot_feature_maps)\n\n loss += regularized_cross_entropy(self.layers, regularization, tmp_output[y[i]])\n\n prediction = np.argmax(tmp_output)\n if prediction == y[i]:\n num_correct += 1\n if plot_correct:\n image = (X[i] * 255)[0, :, :]\n plot_sample(image, y[i], prediction)\n plot_correct = 1\n else:\n if plot_missclassified:\n image = (X[i] * 255)[0, :, :]\n plot_sample(image, y[i], prediction)\n plot_missclassified = 1\n\n test_size = len(X)\n accuracy = (num_correct / test_size) * 100\n loss = loss / test_size\n if verbose:\n print('Test Loss: %02.3f' % loss)\n print('Test Accuracy: %02.3f' % accuracy)\n return loss, accuracy"
    },
    {
      "execution_count": 5,
      "code_cell_id": 4,
      "code": "dataset = load_cifar()\n\nX_train = dataset['train_images']\ny_train = dataset['train_labels']\nX_val = dataset['validation_images']\ny_val = dataset['validation_labels']\nX_test = dataset['test_images']\ny_test = dataset['test_labels']\n\nprint(\"Training data shape:\", X_train.shape)\nprint(\"Training labels shape:\", y_train.shape)\nprint(\"Validation data shape:\", X_val.shape)\nprint(\"Validation labels shape:\", y_val.shape)\nprint(\"Test data shape:\", X_test.shape)\nprint(\"Test labels shape:\", y_test.shape)"
    }
  ],
  "target": {
    "code_cell_id": 5,
    "code": "from keras.regularizers import l2\nfrom keras.optimizers import SGD\nfrom keras.models import Sequential\nfrom keras.layers import Conv2D, MaxPooling2D, Dense, Flatten\nfrom keras.utils import to_categorical\nfrom keras.callbacks import Callback\nfrom keras import Model\nfrom keras.models import load_model\nimport numpy as np\nimport os\nimport matplotlib.pyplot as plt\n\nclass History(Callback):\n def __init__(self, model, validation_images, validation_labels):\n self.model_ = model\n self.validation_images = validation_images\n self.validation_labels = validation_labels\n self.accuracy = [0]\n self.loss = [5]\n self.val_accuracy = [0]\n self.val_loss = [5]\n\n def on_batch_end(self, batch, logs={}):\n scores = self.model_.evaluate(\n self.validation_images,\n self.validation_labels,\n verbose=0\n )\n print('\\n', scores, '\\n')\n self.loss.append(logs.get('loss'))\n self.accuracy.append(logs.get('accuracy'))\n self.val_loss.append(scores[0])\n self.val_accuracy.append(scores[1])\n\ndef train(model, train_images, train_labels, validation_images, validation_labels, batch_size, num_epochs, learning_rate, verbose):\n opt = SGD(learning_rate)\n model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n\n history = History(model, validation_images, validation_labels)\n model.fit(\n train_images,\n train_labels,\n batch_size=batch_size,\n epochs=num_epochs,\n callbacks=[history]\n )\n\n if verbose:\n plot_learning_curve(history.loss)\n plot_accuracy_curve(history.accuracy, history.val_accuracy)\n\ndef evaluate(model):\n scores = model.evaluate(test_images, test_labels, verbose=1)\n print('Test loss:', scores[0])\n print('Test accuracy:', scores[1])\n\ndef predict(model, image_idx):\n layer_names = ['conv1', 'conv2', 'conv3', 'conv4']\n num_features = 4\n\n dataset = load_cifar()\n\n image = test_images[image_idx]\n image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n pred = np.argmax(model.predict(image))\n\n plot_sample(dataset['test_images'][image_idx], classes[dataset['test_labels'][image_idx]], classes[pred])\n\n feature_maps = []\n for name in layer_names:\n tmp_model = Model(inputs=model.input, outputs=model.get_layer(name).output)\n feature_maps.append(tmp_model.predict(image))\n\n fig, ax = plt.subplots(nrows=len(feature_maps), ncols=num_features, figsize=(20, 20))\n for i in range(len(feature_maps)):\n for z in range(num_features):\n ax[i][z].imshow(feature_maps[i][0, :, :, z])\n ax[i][z].set_title(layer_names[i])\n ax[i][z].set_xticks([])\n ax[i][z].set_yticks([])\n plt.savefig('feature_maps.png')\n\ndef plot_weights(model):\n for layer in model.layers:\n if 'conv' in layer.name:\n weights, _ = layer.get_weights()\n plot_histogram(layer.name, np.reshape(weights, -1))\n\nif __name__ == '__main__':\n\n classes = [\n \"airplane\",\n \"automobile\",\n \"bird\",\n \"cat\",\n \"deer\",\n \"dog\",\n \"frog\",\n \"horse\",\n \"ship\",\n \"truck\"\n ]\n\n num_epochs = 1\n learning_rate = 0.005\n batch_size = 2000\n lam = 0.01\n verbose = 1\n\n print('\\n--- Loading mnist dataset ---')\n dataset = load_cifar()\n\n print('\\n--- Processing the dataset ---')\n dataset = preprocess(dataset)\n\n train_images = np.moveaxis(dataset['train_images'], 1, 3)\n validation_images = np.moveaxis(dataset['validation_images'], 1, 3)\n test_images = np.moveaxis(dataset['test_images'], 1, 3)\n train_labels = to_categorical(dataset['train_labels'])\n validation_labels = to_categorical(dataset['validation_labels'])\n test_labels = to_categorical(dataset['test_labels'])\n\n if os.path.isfile('model.h5'):\n print('\\n--- Loading model ---')\n model = load_model('model.h5')\n else:\n print('\\n--- Building model ---')\n model = Sequential()\n model.add(Conv2D(32, 3, name='conv1', activation='relu', kernel_initializer='he_normal', kernel_regularizer=l2(lam), input_shape=(32, 32, 3)))\n model.add(Conv2D(32, 3, name='conv2', activation='relu', kernel_initializer='he_normal', kernel_regularizer=l2(lam)))\n model.add(MaxPooling2D(2, name='pool1'))\n model.add(Conv2D(64, 3, name='conv3', activation='relu', kernel_initializer='he_normal', kernel_regularizer=l2(lam)))\n model.add(Conv2D(64, 3, name='conv4', activation='relu', kernel_initializer='he_normal', kernel_regularizer=l2(lam)))\n model.add(MaxPooling2D(2, name='pool2'))\n model.add(Flatten())\n model.add(Dense(256, name='fullyconnected', activation='relu', kernel_initializer='he_normal', kernel_regularizer=l2(lam)))\n model.add(Dense(10, name='dense', activation='softmax'))\n\n train_images = np.moveaxis(train_images, -1, 1)\n validation_images = np.moveaxis(validation_images, -1, 1)\n test_images = np.moveaxis(test_images, -1, 1)\n\n train(\n model,\n train_images,\n train_labels,\n validation_images,\n validation_labels,\n batch_size,\n num_epochs,\n learning_rate,\n verbose\n )\n\n print('\\n--- Testing the model ---')\n evaluate(model)\n\n print('\\n--- Predicting image from test set ---')\n image_idx = 40\n predict(model, image_idx)\n\n print('\\n--- Plotting weight distributions ---')\n plot_weights(model)\n\n print('\\n--- Saving the model ---')\n model.save('model.h5')"
  }
}