{
  "executed": [
    {
      "execution_count": 1,
      "code_cell_id": 0,
      "code": "import numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nimport cv2"
    },
    {
      "execution_count": 5,
      "code_cell_id": 1,
      "code": "import os\nos.listdir('data_small/dataset/train')"
    },
    {
      "execution_count": 6,
      "code_cell_id": 3,
      "code": "def takeFileName(filedir):\n\n filename = np.array(filedir.split('/'))[-1]\n\n return filename"
    },
    {
      "execution_count": 7,
      "code_cell_id": 4,
      "code": "train_path_watermarked_images = 'data_small/dataset/train/watermark/'\ntrain_path_nonwatermarked_images = 'data_small/dataset/train/no_watermark/'"
    },
    {
      "execution_count": 8,
      "code_cell_id": 5,
      "code": "tp_watermarked = np.array([])\ntp_nonwatermarked = np.array([])\n\nfor root, dirs, files in os.walk(train_path_watermarked_images, topdown=True):\n for file in files:\n tp_watermarked = np.append(tp_watermarked, takeFileName(file))\n\nfor root, dirs, files in os.walk(train_path_nonwatermarked_images, topdown=True):\n for file in files:\n tp_nonwatermarked = np.append(tp_nonwatermarked, takeFileName(file))"
    },
    {
      "execution_count": 9,
      "code_cell_id": 6,
      "code": "output_array_wm = []\n\nfor i in tp_watermarked:\n output_string_wm = train_path_watermarked_images + i\n output_array_wm.append(output_string_wm)\n out_array_wm=np.array(output_array_wm)"
    },
    {
      "execution_count": 10,
      "code_cell_id": 7,
      "code": "output_array_nwm = []\n\nfor i in tp_nonwatermarked:\n output_string_nwm = train_path_nonwatermarked_images + i\n output_array_nwm.append(output_string_nwm)\n out_array_nwm=np.array(output_array_nwm)"
    },
    {
      "execution_count": 11,
      "code_cell_id": 10,
      "code": "width = 196\nheight = 196\ndim = (width, height)\ndef createPixelArr(files):\n data = []\n for image in files:\n try:\n img_arr = cv2.imread(image, cv2.IMREAD_COLOR)\n img_arr = cv2.cvtColor(img_arr, cv2.COLOR_BGR2RGB)\n resized_arr = cv2.resize(img_arr, (width, height))\n data.append(resized_arr)\n except Exception as e:\n print(e)\n return np.array(data)"
    },
    {
      "execution_count": 12,
      "code_cell_id": 11,
      "code": "train_wms_pixVals = createPixelArr(out_array_wm[:90])\ntrain_nwms_pixVals = createPixelArr(out_array_nwm[:90])"
    },
    {
      "execution_count": 13,
      "code_cell_id": 12,
      "code": "X_train, X_test, y_train, y_test = train_test_split(train_wms_pixVals, train_nwms_pixVals, train_size=0.8, random_state=1)"
    },
    {
      "execution_count": 14,
      "code_cell_id": 15,
      "code": "import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom tqdm import tqdm\nimport numpy as np\nimport torchvision\nfrom torchvision import datasets, models, transforms\nfrom torch.utils.data import DataLoader, Dataset\nimport matplotlib.pyplot as plt\nimport time\nimport os\nimport copy\nfrom PIL import Image\nimport pandas as pd\nimport random\nfrom tqdm import tqdm\nimport timm\nimport torch.nn.functional as F\nfrom timm.models.layers import trunc_normal_, DropPath\nfrom timm.models.registry import register_model"
    },
    {
      "execution_count": 15,
      "code_cell_id": 20,
      "code": "model_ft = timm.create_model(\n 'efficientnet_b3a', pretrained=True, num_classes=2\n)\nmodel_ft.classifier = nn.Sequential(\n nn.Linear(in_features=1536, out_features=625),\n nn.ReLU(),\n nn.Dropout(p=0.3),\n nn.Linear(in_features=625, out_features=256),\n nn.ReLU(),\n nn.Linear(in_features=256, out_features=2),\n)"
    },
    {
      "execution_count": 16,
      "code_cell_id": 22,
      "code": "device = torch.device('cpu')\n\ndef train_model(model, train_loader, test_loader, criterion, optimizer, num_epochs=80):\n since = time.time()\n\n val_acc_history = []\n train_acc_history = []\n\n best_model_wts = copy.deepcopy(model.state_dict())\n best_acc = 0.0\n\n for epoch in range(num_epochs):\n print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n print('-' * 10)\n\n model.train()\n\n running_loss = 0.0\n running_corrects = 0\n\n for inputs, labels in tqdm(train_loader):\n inputs = inputs.to(device)\n labels = labels.to(device)\n\n optimizer.zero_grad()\n\n with torch.set_grad_enabled(True):\n with torch.cuda.amp.autocast():\n outputs = model(inputs)\n loss = criterion(outputs, labels)\n\n _, preds = torch.max(outputs, 1)\n\n loss.backward()\n optimizer.step()\n\n running_loss += loss.item() * inputs.size(0)\n running_corrects += torch.sum(preds == labels.data)\n\n epoch_loss = running_loss / len(train_loader.dataset)\n epoch_acc = running_corrects.double() / len(train_loader.dataset)\n\n print('Train Loss: {:.4f} Acc: {:.4f}'.format(epoch_loss, epoch_acc))\n train_acc_history.append(epoch_acc)\n\n model.eval()\n\n running_loss = 0.0\n running_corrects = 0\n\n for inputs, labels in tqdm(test_loader):\n inputs = inputs.to(device)\n labels = labels.to(device)\n\n with torch.set_grad_enabled(False):\n with torch.cuda.amp.autocast():\n outputs = model(inputs)\n loss = criterion(outputs, labels)\n\n _, preds = torch.max(outputs, 1)\n\n running_loss += loss.item() * inputs.size(0)\n running_corrects += torch.sum(preds == labels.data)\n\n epoch_loss = running_loss / len(test_loader.dataset)\n epoch_acc = running_corrects.double() / len(test_loader.dataset)\n\n print('Test Loss: {:.4f} Acc: {:.4f}'.format(epoch_loss, epoch_acc))\n val_acc_history.append(epoch_acc)\n\n if epoch_acc > best_acc:\n best_acc = epoch_acc\n best_model_wts = copy.deepcopy(model.state_dict())\n\n print()\n\n time_elapsed = time.time() - since\n print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n print('Best val Acc: {:4f}'.format(best_acc))\n\n model.load_state_dict(best_model_wts)\n return model, train_acc_history, val_acc_history"
    },
    {
      "execution_count": 17,
      "code_cell_id": 23,
      "code": "criterion = torch.nn.CrossEntropyLoss()\noptimizer = optim.AdamW(params=model_ft.parameters(), lr=0.2e-5)"
    },
    {
      "execution_count": 18,
      "code_cell_id": 24,
      "code": "class MyDataset(Dataset):\n def __init__(self, X, y):\n self.X = X\n self.y = y\n\n def __len__(self):\n return len(self.X)\n\n def __getitem__(self, idx):\n return self.X[idx], self.y[idx]"
    },
    {
      "execution_count": 19,
      "code_cell_id": 25,
      "code": "train_dataset = MyDataset(X_train, y_train)\ntrain_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)"
    },
    {
      "execution_count": 20,
      "code_cell_id": 26,
      "code": "test_dataset = MyDataset(X_test, y_test)\ntest_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)"
    }
  ],
  "target": {
    "code_cell_id": 27,
    "code": "import warnings\nwarnings.filterwarnings(\"ignore\")\n\nmodel_ft, train_acc_history, val_acc_history = train_model(\n model_ft, train_loader, test_loader, criterion, optimizer, num_epochs=3\n)"
  }
}