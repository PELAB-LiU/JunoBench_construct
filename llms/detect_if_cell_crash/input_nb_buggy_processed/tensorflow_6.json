{
  "executed": [
    {
      "execution_count": 3,
      "code_cell_id": 0,
      "code": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nimport seaborn as sns\n%matplotlib inline\n\nnp.random.seed(2)\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\n\nimport itertools\n\nfrom keras.utils import to_categorical\n\nfrom keras.models import Sequential\n\nfrom keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D\n\nfrom keras.optimizers import RMSprop\n\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\n\nfrom keras.callbacks import ReduceLROnPlateau\n\nsns.set(style='white', context='notebook', palette='deep')"
    },
    {
      "execution_count": 4,
      "code_cell_id": 1,
      "code": "import tensorflow as tf"
    },
    {
      "execution_count": 5,
      "code_cell_id": 3,
      "code": "train = pd.read_csv(\"data/train.csv\")\ntest = pd.read_csv(\"data/test.csv\")"
    },
    {
      "execution_count": 6,
      "code_cell_id": 4,
      "code": "Y_train = train[\"label\"]\n\nX_train = train.drop(labels = [\"label\"],axis = 1)\n\ndel train\n\ng = sns.countplot(Y_train)\n\nY_train.value_counts()"
    },
    {
      "execution_count": 7,
      "code_cell_id": 6,
      "code": "X_train = X_train / 255.0\ntest = test / 255.0"
    },
    {
      "execution_count": 8,
      "code_cell_id": 8,
      "code": "X_train = X_train.values.reshape(-1,28,28,1)\ntest = test.values.reshape(-1,28,28,1)"
    },
    {
      "execution_count": 9,
      "code_cell_id": 9,
      "code": "Y_train = to_categorical(Y_train, num_classes = 10)"
    },
    {
      "execution_count": 10,
      "code_cell_id": 11,
      "code": "random_seed = 2\nX_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size = 0.1, random_state=random_seed)"
    },
    {
      "execution_count": 17,
      "code_cell_id": 13,
      "code": "'''\n- filters：滤波器的数量，即输出的通道数；\n- kernelsize：卷积核的大小，这里是一个5x5的矩阵；\n- padding：卷积的方式，这里使用'Same'表示输出图像的大小与输入图像相同；\n- activation：激活函数，这里使用'Relu'函数；\n- inputshape：输入数据的形状，这里是一个28x28的灰度图像（深度为1）。\n'''\n\nimport keras\nmodel = keras.models.Sequential([\n Conv2D(filters=32, kernel_size=(3, 3), padding='same', activation='relu', input_shape=(28, 28, 1)),\n])\nprint(X_train[0].shape)\np = model.predict(X_train[0:3])\np.shape"
    },
    {
      "execution_count": 18,
      "code_cell_id": 17,
      "code": "model = Sequential()\n\nmodel.add(Conv2D(filters = 32, kernel_size = (5,5),padding = 'Same',\n activation ='relu', input_shape = (28,28,1)))\n\nmodel.add(Conv2D(filters = 32, kernel_size = (5,5),padding = 'Same',\n activation ='relu'))\n\nmodel.add(MaxPool2D(pool_size=(2,2)))\n\nmodel.add(Dropout(0.25))\n\nmodel.add(Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same',\n activation ='relu'))\n\nmodel.add(Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same',\n activation ='relu'))\n\nmodel.add(MaxPool2D(pool_size=(2,2), strides=(2,2)))\n\nmodel.add(Dropout(0.25))\n\nmodel.add(Flatten())\n\nmodel.add(Dense(256, activation = \"relu\"))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(10, activation = \"softmax\"))"
    },
    {
      "execution_count": 19,
      "code_cell_id": 19,
      "code": "optimizer = RMSprop(learning_rate=0.001, rho=0.9, epsilon=1e-08, decay=0.0)\n'''\n“decay=0”是RMSprop优化算法的一个参数，它控制了学习率的衰减。\n具体来说，在RMSprop算法中，每个权重参数都有自己的学习率\n，而decay参数会使这个学习率随着时间的推移而逐渐变小。\n但是，由于decay=0，因此该算法不会在训练过程中降低学习率。\n\n“epsilon=1e-08”是RMSprop优化算法的一个参数，它是用来防止除零错误的一个小量，通常取极小的值（例如1e-8）。\n在RMSprop算法中，计算梯度平方平均值时需要对平方梯度加上一个极小的值，以避免出现除以零的错误。\n\n“rho=0.9”是一种优化算法的参数，用于控制梯度的平滑度。具体来说，该参数决定了在计算平方梯度的指数移动平均值时，\n历史数据的重要性。较高的rho值可以使平均值对历史数据的依赖性更强，从而使整个优化过程更加稳定。\n'''"
    },
    {
      "execution_count": 20,
      "code_cell_id": 20,
      "code": "model.compile(optimizer = optimizer , loss = \"categorical_crossentropy\", metrics=[\"accuracy\"])"
    },
    {
      "execution_count": 21,
      "code_cell_id": 21,
      "code": "learning_rate_reduction = ReduceLROnPlateau(monitor='val_acc',\n patience=3,\n verbose=1,\n factor=0.5,\n min_lr=0.00001)\n'''\n\n在您提供的代码中，learningratereduction是ReduceLROnPlateau回调函数的一个实例，用于在模型训练过程中动态地减小学习率。\n具体来说，它监视了验证集的准确性（即'monitor'='valacc'），并且如果在3个时期内没有改进，\n则减小学习率（即'patience'=3）。该调用还指定了减小因子（即'factor'=0.5）和最小学习率（即'minlr'=0.00001），\n以便在执行减少操作时进行限制，从而保持学习率的稳定性和有效性。如果您想要更好地了解ReduceLROnPlateau的工作原理和参数设置，\n\nverbose是ReduceLROnPlateau回调函数的一个可选参数，用于控制输出详细程度的标志。\n如果verbose=1，则在执行时期减少操作时将输出一条消息，以指示学习率的更新和当前的状态。\n如果verbose=0，则不会输出任何消息。通常情况下，verbose的默认值为0，因为它可以大大减少输出的噪声和干扰。\n如果您需要更详细的输出和信息，可以将verbose的值设置为1或更高\n可以查看Keras文档：https://keras.io/callbacks/#reducelronplateau。\n'''"
    },
    {
      "execution_count": 23,
      "code_cell_id": 22,
      "code": "checkpoint_filepath = 'best_model.keras'\nmodel_checkpoint_callback = keras.callbacks.ModelCheckpoint(\n filepath=checkpoint_filepath,\n monitor='val_acc',\n mode='max',\n save_best_only=True)"
    },
    {
      "execution_count": 24,
      "code_cell_id": 23,
      "code": "epochs = 1\nbatch_size = 86"
    },
    {
      "execution_count": 25,
      "code_cell_id": 26,
      "code": "datagen = ImageDataGenerator(\n featurewise_center=False,\n samplewise_center=False,\n featurewise_std_normalization=False,\n samplewise_std_normalization=False,\n\n zca_whitening=False,\n rotation_range=10,\n zoom_range = 0.1,\n width_shift_range=0.1,\n height_shift_range=0.1,\n\n horizontal_flip=False,\n vertical_flip=False)\n\ndatagen.fit(X_train)"
    },
    {
      "execution_count": 27,
      "code_cell_id": 27,
      "code": "history = model.fit(datagen.flow(X_train,Y_train, batch_size=batch_size),\n epochs = epochs, validation_data = (X_val,Y_val),\n verbose = 2, steps_per_epoch=X_train.shape[0] // batch_size\n , callbacks=[learning_rate_reduction,model_checkpoint_callback])"
    }
  ],
  "target": {
    "code_cell_id": 29,
    "code": "fig, ax = plt.subplots(2,1)\nax[0].plot(history.history['loss'], color='b', label=\"Training loss\")\nax[0].plot(history.history['val_loss'], color='r', label=\"validation loss\",axes =ax[0])\nlegend = ax[0].legend(loc='best', shadow=True)\n\nax[1].plot(history.history['acc'], color='b', label=\"Training accuracy\")\nax[1].plot(history.history['val_acc'], color='r',label=\"Validation accuracy\")\nlegend = ax[1].legend(loc='best', shadow=True)"
  }
}