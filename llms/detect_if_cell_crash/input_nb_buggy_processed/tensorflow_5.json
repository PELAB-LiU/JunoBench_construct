{
  "executed": [
    {
      "execution_count": 1,
      "code_cell_id": 0,
      "code": "INPUT_DIR = 'data'\n!ls {INPUT_DIR}"
    },
    {
      "execution_count": 2,
      "code_cell_id": 1,
      "code": "import numpy as np\nimport pandas as pd\n\nrating_df = pd.read_csv(INPUT_DIR + '/rating_complete.csv',\n low_memory=False,\n usecols=[\"user_id\", \"anime_id\", \"rating\"]\n )\nrating_df.head(4)"
    },
    {
      "execution_count": 3,
      "code_cell_id": 2,
      "code": "n_ratings = rating_df['user_id'].value_counts()\nrating_df = rating_df[rating_df['user_id'].isin(n_ratings[n_ratings >= 400].index)].copy()\nlen(rating_df)"
    },
    {
      "execution_count": 4,
      "code_cell_id": 3,
      "code": "rating_df['rating'].isna().sum()"
    },
    {
      "execution_count": 5,
      "code_cell_id": 4,
      "code": "min_rating = min(rating_df['rating'])\nmax_rating = max(rating_df['rating'])\nrating_df['rating'] = rating_df[\"rating\"].apply(lambda x: (x - min_rating) / (max_rating - min_rating)).values.astype(np.float64)\n\nAvgRating = np.mean(rating_df['rating'])\nprint('Avg', AvgRating)"
    },
    {
      "execution_count": 6,
      "code_cell_id": 8,
      "code": "user_ids = rating_df[\"user_id\"].unique().tolist()[:1000]\nuser2user_encoded = {x: i for i, x in enumerate(user_ids)}\nuser_encoded2user = {i: x for i, x in enumerate(user_ids)}\nrating_df[\"user\"] = rating_df[\"user_id\"].map(user2user_encoded)\nn_users = len(user2user_encoded)\n\nanime_ids = rating_df[\"anime_id\"].unique().tolist()[:1000]\nanime2anime_encoded = {x: i for i, x in enumerate(anime_ids)}\nanime_encoded2anime = {i: x for i, x in enumerate(anime_ids)}\nrating_df[\"anime\"] = rating_df[\"anime_id\"].map(anime2anime_encoded)\nn_animes = len(anime2anime_encoded)\n\nprint(\"Num of users: {}, Num of animes: {}\".format(n_users, n_animes))\nprint(\"Min rating: {}, Max rating: {}\".format(min(rating_df['rating']), max(rating_df['rating'])))"
    },
    {
      "execution_count": 7,
      "code_cell_id": 9,
      "code": "rating_df = rating_df.sample(frac=1, random_state=73)\n\nrating_df= rating_df.head(1000)\n\nX = rating_df[['user', 'anime']].values\ny = rating_df[\"rating\"]"
    },
    {
      "execution_count": 8,
      "code_cell_id": 10,
      "code": "rating_df.shape[0]"
    },
    {
      "execution_count": 9,
      "code_cell_id": 11,
      "code": "from sklearn.model_selection import train_test_split\n\nlimit_rows = 1000\n\ntest_set_size = int(0.2 * rating_df.shape[0])\ntrain_indices = rating_df.shape[0] - test_set_size\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nprint('> Train set ratings: {}'.format(len(y_train)))\nprint('> Test set ratings: {}'.format(len(y_test)))\nprint(len(X_train))\nprint(len(X_test))"
    },
    {
      "execution_count": 10,
      "code_cell_id": 12,
      "code": "X_train_array = [X_train[:, 0], X_train[:, 1]]\nX_test_array = [X_test[:, 0], X_test[:, 1]]"
    },
    {
      "execution_count": 11,
      "code_cell_id": 14,
      "code": "import tensorflow as tf\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam"
    },
    {
      "execution_count": 12,
      "code_cell_id": 15,
      "code": "from tensorflow.keras.layers import Add, Activation, Lambda, BatchNormalization, Concatenate, Dropout, Input, Embedding, Dot, Reshape, Dense, Flatten\n\ndef RecommenderNet():\n embedding_size = 128\n\n user = Input(name = 'user', shape = [1])\n user_embedding = Embedding(name = 'user_embedding',\n input_dim = n_users,\n output_dim = embedding_size)(user)\n\n anime = Input(name = 'anime', shape = [1])\n anime_embedding = Embedding(name = 'anime_embedding',\n input_dim = n_animes,\n output_dim = embedding_size)(anime)\n\n x = Dot(name = 'dot_product', normalize = True, axes = 2)([user_embedding, anime_embedding])\n x = Flatten()(x)\n\n x = Dense(1, kernel_initializer='he_normal')(x)\n x = BatchNormalization()(x)\n x = Activation(\"sigmoid\")(x)\n\n model = Model(inputs=[user, anime], outputs=x)\n model.compile(loss='binary_crossentropy', metrics=[\"mae\", \"mse\"], optimizer='adam')\n\n return model\n\nmodel = RecommenderNet()\n\nmodel.summary()"
    },
    {
      "execution_count": 13,
      "code_cell_id": 16,
      "code": "from tensorflow.keras.callbacks import Callback, ModelCheckpoint, LearningRateScheduler, TensorBoard, EarlyStopping, ReduceLROnPlateau\n\nstart_lr = 0.00001\nmin_lr = 0.00001\nmax_lr = 0.00005\nbatch_size = 100\n\nrampup_epochs = 5\nsustain_epochs = 0\nexp_decay = .8\n\ndef lrfn(epoch):\n if epoch < rampup_epochs:\n return (max_lr - start_lr)/rampup_epochs * epoch + start_lr\n elif epoch < rampup_epochs + sustain_epochs:\n return max_lr\n else:\n return (max_lr - min_lr) * exp_decay**(epoch-rampup_epochs-sustain_epochs) + min_lr\n\nlr_callback = LearningRateScheduler(lambda epoch: lrfn(epoch), verbose=0)\n\ncheckpoint_filepath = 'weights.weights.h5'\n\nmodel_checkpoints = ModelCheckpoint(filepath=checkpoint_filepath,\n save_weights_only=True,\n monitor='val_loss',\n mode='min',\n save_best_only=True)\n\nearly_stopping = EarlyStopping(patience = 3, monitor='val_loss',\n mode='min', restore_best_weights=True)\n\nmy_callbacks = [\n model_checkpoints,\n lr_callback,\n early_stopping,\n]"
    },
    {
      "execution_count": 14,
      "code_cell_id": 17,
      "code": "print(X_train_array)"
    }
  ],
  "target": {
    "code_cell_id": 18,
    "code": "history = model.fit(\n x=X_train_array,\n y=y_train,\n batch_size=batch_size,\n epochs=5,\n verbose=1,\n validation_data=(X_test_array, y_test),\n callbacks=my_callbacks\n)"
  }
}