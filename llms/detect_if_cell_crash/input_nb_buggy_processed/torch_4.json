{
  "executed": [
    {
      "execution_count": 1,
      "code_cell_id": 0,
      "code": "import pandas as pd\nimport numpy as np\nimport warnings\nwarnings.filterwarnings('ignore')"
    },
    {
      "execution_count": 5,
      "code_cell_id": 8,
      "code": "import logging\nimport random\n\nimport torch\n\nlogging.basicConfig(level=logging.INFO, format='%(asctime)-15s %(levelname)s: %(message)s')\n\nseed = 2023\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.cuda.manual_seed(seed)\ntorch.manual_seed(seed)\n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'"
    },
    {
      "execution_count": 7,
      "code_cell_id": 9,
      "code": "from gensim.models.word2vec import Word2Vec\n\nnum_features = 10\nnum_workers = 8\n\ntrain_df = pd.read_csv('data/train_set.csv.zip', sep='\\t', nrows=5000)\ntrain_text = list(map(lambda x:list(x.split()), train_df.iloc[:, 1]))\nmodel = Word2Vec(train_text, workers=num_workers, vector_size=num_features)\nmodel.init_sims(replace=True)\n\nmodel.wv.save_word2vec_format('data/word2vec.txt', binary=False)"
    },
    {
      "execution_count": 8,
      "code_cell_id": 11,
      "code": "from collections import Counter\nfrom transformers import BasicTokenizer\n\nbasic_tokenizer = BasicTokenizer()\n\nclass Vocab():\n def __init__(self, train_data):\n self.min_count = 5\n self.pad = 0\n self.unk = 1\n self._id2word = ['[PAD]', '[UNK]']\n self._id2extword = ['[PAD]', '[UNK]']\n\n self._id2label = []\n self.target_names = []\n\n self.build_vocab(train_data)\n\n reverse = lambda x: dict(zip(x, range(len(x))))\n self._word2id = reverse(self._id2word)\n self._label2id = reverse(self._id2label)\n\n def build_vocab(self, data):\n self.word_counter = Counter()\n\n for text in data['text']:\n words = text.split()\n for word in words:\n self.word_counter[word] += 1\n\n for word, count in self.word_counter.most_common():\n if count >= self.min_count:\n self._id2word.append(word)\n\n label2name = {0: '科技', 1: '股票', 2: '体育', 3: '娱乐', 4: '时政', 5: '社会', 6: '教育',\n 7: '财经', 8: '家居', 9: '游戏', 10: '房产', 11: '时尚', 12: '彩票', 13: '星座'}\n self.label_counter = Counter(data['label'])\n\n for label in range(len(self.label_counter)):\n count = self.label_counter[label]\n self._id2label.append(label)\n self.target_names.append(label2name[label])\n\n def load_pretrained_embs(self, embfile):\n with open(embfile, encoding='utf-8') as f:\n lines = f.readlines()\n items = lines[0].split()\n word_count, embedding_dim = int(items[0]), int(items[1])\n\n index = len(self._id2extword)\n embeddings = np.zeros((word_count + index, embedding_dim))\n for line in lines[1:]:\n values = line.split()\n self._id2extword.append(values[0])\n vector = np.array(values[1:], dtype='float64')\n embeddings[self.unk] += vector\n embeddings[index] = vector\n index += 1\n\n embeddings[self.unk] = embeddings[self.unk] / word_count\n embeddings = embeddings / np.std(embeddings)\n\n reverse = lambda x: dict(zip(x, range(len(x))))\n self._extword2id = reverse(self._id2extword)\n\n assert len(set(self._id2extword)) == len(self._id2extword)\n\n return embeddings\n\n def word2id(self, xs):\n if isinstance(xs, list):\n return [self._word2id.get(x, self.unk) for x in xs]\n return self._word2id.get(xs, self.unk)\n\n def extword2id(self, xs):\n if isinstance(xs, list):\n return [self._extword2id.get(x, self.unk) for x in xs]\n return self._extword2id.get(xs, self.unk)\n\n def label2id(self, xs):\n if isinstance(xs, list):\n return [self._label2id.get(x, self.unk) for x in xs]\n return self._label2id.get(xs, self.unk)\n\n def word_size(self):\n return len(self._id2word)\n\n def extword_size(self):\n return len(self._id2extword)\n\n def label_size(self):\n return len(self._id2label)\n\nvocab = Vocab(train_df)"
    },
    {
      "execution_count": 9,
      "code_cell_id": 12,
      "code": "import torch.nn as nn\nimport torch.nn.functional as F\n\nclass Attention(nn.Module):\n '''Scaled Dot-Product Attention'''\n def __init__(self, hidden_size):\n super(Attention, self).__init__()\n self.weight = nn.Parameter(torch.Tensor(hidden_size, hidden_size))\n self.weight.data.normal_(mean=0.0, std=0.05)\n\n self.bias = nn.Parameter(torch.Tensor(hidden_size))\n b = np.zeros(hidden_size, dtype=np.float32)\n self.bias.data.copy_(torch.from_numpy(b))\n\n self.query = nn.Parameter(torch.Tensor(hidden_size))\n self.query.data.normal_(mean=0.0, std=0.05)\n\n def forward(self, batch_hidden, batch_masks):\n\n key = torch.matmul(batch_hidden, self.weight) + self.bias\n\n outputs = torch.matmul(key, self.query)\n\n masked_outputs = outputs.masked_fill((1 - batch_masks).bool(), float(-1e32))\n\n attn_scores = F.softmax(masked_outputs, dim=1)\n\n masked_attn_scores = attn_scores.masked_fill((1 - batch_masks).bool(), 0.0)\n\n batch_outputs = torch.bmm(masked_attn_scores.unsqueeze(1), key).squeeze(1)\n\n return batch_outputs, attn_scores"
    },
    {
      "execution_count": 10,
      "code_cell_id": 13,
      "code": "word2vec_path = 'data/word2vec.txt'\ndropout = 0.15\nword_hidden_size = 128\nword_num_layers = 2\n\nclass WordLSTMEncoder(nn.Module):\n '''\n 结合word2vec（预训练）和nn.Embedding()（待训练）的词向量表示，然后进一步用lstm提取序列信息，更新词向量表示\n '''\n def __init__(self, vocab):\n super(WordLSTMEncoder, self).__init__()\n self.dropout = nn.Dropout(dropout)\n self.word_dims = num_features\n\n self.word_embed = nn.Embedding(vocab.word_size, self.word_dims, padding_idx=0)\n\n extword_embed = vocab.load_pretrained_embs(word2vec_path)\n extword_size, word_dims = extword_embed.shape\n\n self.extword_embed = nn.Embedding(extword_size, word_dims, padding_idx=0)\n self.extword_embed.weight.data.copy_(torch.from_numpy(extword_embed))\n self.extword_embed.weight.requires_grad = False\n\n input_size = self.word_dims\n\n self.word_lstm = nn.LSTM(input_size=input_size,\n hidden_size=word_hidden_size,\n num_layers=word_num_layers,\n batch_first=True,\n bidirectional=True)\n\n def forward(self, word_ids, extword_ids, batch_masks):\n\n word_embed = self.word_embed(word_ids)\n extword_embed = self.extword_embed(extword_ids)\n batch_embed = word_embed + extword_embed\n\n if self.training:\n batch_embed = self.dropout(batch_embed)\n\n hiddens, _ = self.word_lstm(batch_embed)\n hiddens = hiddens * batch_masks.unsqueeze(2)\n\n if self.training:\n hiddens = self.dropout(hiddens)\n\n return hiddens"
    },
    {
      "execution_count": 11,
      "code_cell_id": 14,
      "code": "sent_hidden_size = 256\nsent_num_layers = 2\n\nclass SentEncoder(nn.Module):\n '''句子级别的语义表示'''\n def __init__(self, sent_rep_size):\n super(SentEncoder, self).__init__()\n self.dropout = nn.Dropout(dropout)\n\n self.sent_lstm = nn.LSTM(input_size=sent_rep_size,\n hidden_size=sent_hidden_size,\n num_layers=sent_num_layers,\n batch_first=True,\n bidirectional=True)\n\n def forward(self, sent_reps, sent_masks):\n\n sent_hiddens, _ = self.sent_lstm(sent_reps)\n sent_hiddens = sent_hiddens * sent_masks.unsqueeze(2)\n\n if self.training:\n sent_hiddens = self.dropout(sent_hiddens)\n\n return sent_hiddens"
    },
    {
      "execution_count": 31,
      "code_cell_id": 21,
      "code": "def batch2tensor(batch_data):\n '''\n [[label, doc_len, [[sent_len, [sent_id0, ...], [sent_id1, ...]], ...]], ...]\n '''\n batch_size = len(batch_data)\n doc_labels = []\n doc_lens = []\n doc_max_sent_len = []\n for doc_data in batch_data:\n doc_labels.append(doc_data[0])\n doc_lens.append(doc_data[1])\n sent_lens = [sent_data[0] for sent_data in doc_data[2]]\n max_sent_len = max(sent_lens)\n doc_max_sent_len.append(max_sent_len)\n\n max_doc_len = max(doc_lens)\n max_sent_len = max(doc_max_sent_len)\n\n batch_inputs1 = torch.zeros((batch_size, max_doc_len, max_sent_len), dtype=torch.int64)\n batch_inputs2 = torch.zeros((batch_size, max_doc_len, max_sent_len), dtype=torch.int64)\n batch_masks = torch.zeros((batch_size, max_doc_len, max_sent_len), dtype=torch.float32)\n batch_labels = torch.LongTensor(doc_labels)\n\n for b in range(batch_size):\n for sent_idx in range(doc_lens[b]):\n sent_data = batch_data[b][2][sent_idx]\n for word_idx in range(sent_data[0]):\n batch_inputs1[b, sent_idx, word_idx] = sent_data[1][word_idx]\n batch_inputs2[b, sent_idx, word_idx] = sent_data[2][word_idx]\n batch_masks[b, sent_idx, word_idx] = 1\n\n batch_inputs1 = batch_inputs1.to(device)\n batch_inputs2 = batch_inputs2.to(device)\n batch_masks = batch_masks.to(device)\n batch_labels = batch_labels.to(device)\n\n return (batch_inputs1, batch_inputs2, batch_masks), batch_labels\n\nexample_batch_data = [\n [1, 2, [[2, [10, 20], [30, 40]], [3, [50, 60, 70], [80, 90, 100]]]],\n [2, 2, [[2, [15, 25], [35, 45]], [4, [55, 65, 75, 85], [95, 105, 115, 125]]]],\n]"
    }
  ],
  "target": {
    "code_cell_id": 15,
    "code": "class Model(nn.Module):\n def __init__(self, vocab):\n super(Model, self).__init__()\n self.sent_rep_size = word_hidden_size * 2\n self.doc_rep_size = sent_hidden_size * 2\n self.all_parameters = {}\n\n parameters = []\n self.word_encoder = WordLSTMEncoder(vocab)\n self.word_attention = Attention(self.sent_rep_size)\n\n parameters.extend(list(filter(lambda p: p.requires_grad, self.word_encoder.parameters())))\n parameters.extend(list(filter(lambda p: p.requires_grad, self.word_attention.parameters())))\n self.sent_encoder = SentEncoder(self.sent_rep_size)\n self.sent_attention = Attention(self.doc_rep_size)\n parameters.extend(list(filter(lambda p: p.requires_grad, self.sent_encoder.parameters())))\n parameters.extend(list(filter(lambda p: p.requires_grad, self.sent_attention.parameters())))\n self.out = nn.Linear(self.doc_rep_size, vocab.label_size, bias=True)\n parameters.extend(list(filter(lambda p: p.requires_grad, self.out.parameters())))\n\n self.to(device)\n\n if len(parameters) > 0:\n self.all_parameters['basic_parameters'] = parameters\n\n para_num = sum([np.prod(list(p.size())) for p in self.parameters()])\n\n def forward(self, batch_inputs):\n\n batch_inputs1, batch_inputs2, batch_masks = batch_inputs\n batch_size, max_doc_len, max_sent_len = batch_inputs1.shape[0], batch_inputs1.shape[1], batch_inputs1.shape[2]\n batch_inputs1 = batch_inputs1.view(batch_size * max_doc_len, max_sent_len)\n batch_inputs2 = batch_inputs2.view(batch_size * max_doc_len, max_sent_len)\n batch_masks = batch_masks.view(batch_size * max_doc_len, max_sent_len)\n batch_hiddens = self.word_encoder(batch_inputs1, batch_inputs2, batch_masks)\n sent_reps, atten_scores = self.word_attention(batch_hiddens, batch_masks)\n sent_reps = sent_reps.view(batch_size, max_doc_len, self.sent_rep_size)\n batch_masks = batch_masks.view(batch_size, max_doc_len, max_sent_len)\n sent_masks = batch_masks.bool().any(2).float()\n sent_hiddens = self.sent_encoder(sent_reps, sent_masks)\n doc_reps, atten_scores = self.sent_attention(sent_hiddens, sent_masks)\n batch_outputs = self.out(doc_reps)\n\n return batch_outputs\n\nmodel = Model(vocab)"
  }
}