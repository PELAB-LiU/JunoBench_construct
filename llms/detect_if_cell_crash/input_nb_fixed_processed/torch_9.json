{
  "executed": [
    {
      "execution_count": 1,
      "code_cell_id": 0,
      "code": "import torch\nfrom torch import nn\nfrom tqdm.auto import tqdm\nfrom torchvision import transforms\nfrom torchvision.datasets import MNIST\nfrom torchvision.utils import make_grid\nfrom torch.utils.data import DataLoader\nimport matplotlib.pyplot as plt\nfrom torchvision import datasets, transforms\ntorch.manual_seed(0)\n\ndef show_tensor_images(image_tensor, num_images=25, size=(1, 28, 28)):\n '''\n Function for visualizing images: Given a tensor of images, number of images, and\n size per image, plots and prints the images in an uniform grid.\n '''\n image_tensor = (image_tensor + 1) / 2\n image_unflat = image_tensor.detach().cpu()\n image_grid = make_grid(image_unflat[:num_images], nrow=5)\n plt.imshow(image_grid.permute(1, 2, 0).squeeze())\n plt.show()"
    },
    {
      "execution_count": 2,
      "code_cell_id": 1,
      "code": "class Generator(nn.Module):\n '''\n Generator Class\n Values:\n z_dim: the dimension of the noise vector, a scalar\n im_chan: the number of channels of the output image, a scalar\n (MNIST is black-and-white, so 1 channel is your default)\n hidden_dim: the inner dimension, a scalar\n '''\n def __init__(self, z_dim=10, im_chan=3, hidden_dim=64):\n super(Generator, self).__init__()\n self.z_dim = z_dim\n\n self.gen = nn.Sequential(\n self.make_gen_block(z_dim, hidden_dim * 4),\n self.make_gen_block(hidden_dim * 4, hidden_dim * 2, kernel_size=4, stride=1),\n self.make_gen_block(hidden_dim * 2, hidden_dim),\n self.make_gen_block(hidden_dim, im_chan, kernel_size=4, final_layer=True),\n )\n\n def make_gen_block(self, input_channels, output_channels, kernel_size=3, stride=2, final_layer=False):\n '''\n Function to return a sequence of operations corresponding to a generator block of DCGAN,\n corresponding to a transposed convolution, a batchnorm (except for in the last layer), and an activation.\n Parameters:\n input_channels: how many channels the input feature representation has\n output_channels: how many channels the output feature representation should have\n kernel_size: the size of each convolutional filter, equivalent to (kernel_size, kernel_size)\n stride: the stride of the convolution\n final_layer: a boolean, true if it is the final layer and false otherwise\n (affects activation and batchnorm)\n '''\n\n if not final_layer:\n return nn.Sequential(\n\n nn.ConvTranspose2d(input_channels, output_channels, kernel_size, stride),\n nn.BatchNorm2d(output_channels),\n nn.ReLU(inplace=True)\n\n )\n else:\n return nn.Sequential(\n\n nn.ConvTranspose2d(input_channels, output_channels, kernel_size, stride),\n nn.Tanh(),\n\n )\n\n def unsqueeze_noise(self, noise):\n '''\n Function for completing a forward pass of the generator: Given a noise tensor,\n returns a copy of that noise with width and height = 1 and channels = z_dim.\n Parameters:\n noise: a noise tensor with dimensions (n_samples, z_dim)\n '''\n return noise.view(len(noise), self.z_dim, 1, 1)\n\n def forward(self, noise):\n '''\n Function for completing a forward pass of the generator: Given a noise tensor,\n returns generated images.\n Parameters:\n noise: a noise tensor with dimensions (n_samples, z_dim)\n '''\n x = self.unsqueeze_noise(noise)\n return self.gen(x)\n\ndef get_noise(n_samples, z_dim, device='cpu'):\n '''\n Function for creating noise vectors: Given the dimensions (n_samples, z_dim)\n creates a tensor of that shape filled with random numbers from the normal distribution.\n Parameters:\n n_samples: the number of samples to generate, a scalar\n z_dim: the dimension of the noise vector, a scalar\n device: the device type\n '''\n return torch.randn(n_samples, z_dim, device=device)"
    },
    {
      "execution_count": 3,
      "code_cell_id": 2,
      "code": "class Discriminator(nn.Module):\n '''\n Discriminator Class\n Values:\n im_chan: the number of channels of the output image, a scalar\n (MNIST is black-and-white, so 1 channel is your default)\n hidden_dim: the inner dimension, a scalar\n '''\n def __init__(self, im_chan=3, hidden_dim=16):\n super(Discriminator, self).__init__()\n self.disc = nn.Sequential(\n self.make_disc_block(im_chan, hidden_dim),\n self.make_disc_block(hidden_dim, hidden_dim * 2),\n self.make_disc_block(hidden_dim * 2, 1, final_layer=True),\n )\n\n def make_disc_block(self, input_channels, output_channels, kernel_size=4, stride=2, final_layer=False):\n '''\n Function to return a sequence of operations corresponding to a discriminator block of DCGAN,\n corresponding to a convolution, a batchnorm (except for in the last layer), and an activation.\n Parameters:\n input_channels: how many channels the input feature representation has\n output_channels: how many channels the output feature representation should have\n kernel_size: the size of each convolutional filter, equivalent to (kernel_size, kernel_size)\n stride: the stride of the convolution\n final_layer: a boolean, true if it is the final layer and false otherwise\n (affects activation and batchnorm)\n '''\n\n if not final_layer:\n return nn.Sequential(\n\n nn.Conv2d(input_channels, output_channels, kernel_size, stride),\n nn.BatchNorm2d(output_channels),\n nn.LeakyReLU(0.2, inplace=True)\n\n )\n else:\n return nn.Sequential(\n\n nn.Conv2d(input_channels, output_channels, kernel_size, stride)\n\n )\n\n '''\n Function for completing a forward pass of the discriminator: Given an image tensor,\n returns a 1-dimension tensor representing fake/real.\n Parameters:\n image: a flattened image tensor with dimension (im_dim)\n '''\n def forward(self, image):\n disc_pred = self.disc(image)\n return disc_pred.view(len(disc_pred), -1)"
    },
    {
      "execution_count": 12,
      "code_cell_id": 3,
      "code": "criterion = nn.BCEWithLogitsLoss()\nz_dim = 64\ndisplay_step = 500\nbatch_size = 128\n\nlr = 0.0002\n\nbeta_1 = 0.5\nbeta_2 = 0.999\ndevice = 'cpu'\n\ntrain_transform = transforms.Compose([\n\n transforms.Resize((64, 64)),\n\n transforms.ToTensor(),\n transforms.Normalize((0.5,), (0.5,)),\n])\n\ntrain_dataset = datasets.ImageFolder(root='data_small/eyes data', transform=train_transform)\ndataloader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)"
    },
    {
      "execution_count": 13,
      "code_cell_id": 4,
      "code": "def show_images(images):\n fig, ax = plt.subplots(figsize=(20, 20))\n ax.set_xticks([]); ax.set_yticks([])\n ax.imshow(make_grid(images.detach(), nrow=22).permute(1, 2, 0))\n\ndef show_batch(dl):\n for images, _ in dl:\n show_images(images)\n break"
    },
    {
      "execution_count": 15,
      "code_cell_id": 6,
      "code": "gen = Generator(z_dim).to(device)\ngen_opt = torch.optim.Adam(gen.parameters(), lr=lr, betas=(beta_1, beta_2))\ndisc = Discriminator().to(device)\ndisc_opt = torch.optim.Adam(disc.parameters(), lr=lr, betas=(beta_1, beta_2))\n\ndef weights_init(m):\n if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n torch.nn.init.normal_(m.weight, 0.0, 0.02)\n if isinstance(m, nn.BatchNorm2d):\n torch.nn.init.normal_(m.weight, 0.0, 0.02)\n torch.nn.init.constant_(m.bias, 0)\ngen = gen.apply(weights_init)\ndisc = disc.apply(weights_init)"
    },
    {
      "execution_count": 16,
      "code_cell_id": 7,
      "code": "n_epochs = 2\ncur_step = 0\nmean_generator_loss = 0\nmean_discriminator_loss = 0\nfor epoch in range(n_epochs):\n\n for real, _ in tqdm(dataloader):\n cur_batch_size = len(real)\n real = real.to(device)\n\n disc_opt.zero_grad()\n fake_noise = get_noise(cur_batch_size, z_dim, device=device)\n fake = gen(fake_noise)\n disc_fake_pred = disc(fake.detach())\n disc_fake_loss = criterion(disc_fake_pred, torch.zeros_like(disc_fake_pred))\n disc_real_pred = disc(real)\n disc_real_loss = criterion(disc_real_pred, torch.ones_like(disc_real_pred))\n disc_loss = (disc_fake_loss + disc_real_loss) / 2\n\n mean_discriminator_loss += disc_loss.item() / display_step\n\n disc_loss.backward(retain_graph=True)\n\n disc_opt.step()\n\n gen_opt.zero_grad()\n fake_noise_2 = get_noise(cur_batch_size, z_dim, device=device)\n fake_2 = gen(fake_noise_2)\n disc_fake_pred = disc(fake_2)\n gen_loss = criterion(disc_fake_pred, torch.ones_like(disc_fake_pred))\n gen_loss.backward()\n gen_opt.step()\n\n mean_generator_loss += gen_loss.item() / display_step\n\n if cur_step % display_step == 0 and cur_step > 0:\n print(f\"Step {cur_step}: Generator loss: {mean_generator_loss}, discriminator loss: {mean_discriminator_loss}\")\n show_tensor_images(fake)\n show_tensor_images(real)\n mean_generator_loss = 0\n mean_discriminator_loss = 0\n cur_step += 1"
    }
  ],
  "target": {
    "code_cell_id": 5,
    "code": "show_batch(dataloader)"
  }
}