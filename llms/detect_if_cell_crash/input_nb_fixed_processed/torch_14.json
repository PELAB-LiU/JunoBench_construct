{
  "executed": [],
  "target": {
    "code_cell_id": 1,
    "code": "from torchvision.datasets import VOCDetection\nfrom torchvision.transforms import functional as F\nfrom torch.utils.data import Subset\n\nclass VOCDatasetWrapper(VOCDetection):\n def __init__(self, *args, **kwargs):\n super().__init__(*args, **kwargs)\n\n def __getitem__(self, index):\n image, target = super().__getitem__(index)\n\n image = F.to_tensor(image)\n image = F.normalize(image, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n\n boxes = []\n labels = []\n for obj in target['annotation']['object']:\n bbox = obj['bndbox']\n box = [float(bbox['xmin']), float(bbox['ymin']), float(bbox['xmax']), float(bbox['ymax'])]\n boxes.append(box)\n labels.append(int(VOC_CLASSES.index(obj['name'])))\n\n target = {\n 'boxes': torch.tensor(boxes, dtype=torch.float32),\n 'labels': torch.tensor(labels, dtype=torch.int64)\n }\n\n return image, target\n\ndef collate_fn(batch):\n return tuple(zip(*batch))\n\nVOC_CLASSES = [\n 'aeroplane', 'bicycle', 'bird', 'boat', 'bottle',\n 'bus', 'car', 'cat', 'chair', 'cow', 'diningtable',\n 'dog', 'horse', 'motorbike', 'person', 'pottedplant',\n 'sheep', 'sofa', 'train', 'tvmonitor'\n]"
  }
}