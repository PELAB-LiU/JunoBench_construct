{
  "executed": [
    {
      "execution_count": 1,
      "code_cell_id": 0,
      "code": "import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nimport math\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n for filename in filenames:\n print(os.path.join(dirname, filename))"
    },
    {
      "execution_count": 2,
      "code_cell_id": 1,
      "code": "with open('data/raw/txt/romance/domCasmurro.txt', 'r', encoding='utf8') as f:\n data = f.read()"
    },
    {
      "execution_count": 3,
      "code_cell_id": 2,
      "code": "print(\"Extract: \", data[:50])"
    },
    {
      "execution_count": 4,
      "code_cell_id": 3,
      "code": "print(\"Length: \", len(data))"
    },
    {
      "execution_count": 5,
      "code_cell_id": 4,
      "code": "chars = list(set(data))"
    },
    {
      "execution_count": 6,
      "code_cell_id": 5,
      "code": "indexer = {char: index for (index, char) in enumerate(chars)}"
    },
    {
      "execution_count": 7,
      "code_cell_id": 6,
      "code": "indexed_data = []\nfor c in data:\n indexed_data.append(indexer[c])\n\nprint(\"Indexed extract: \", indexed_data[:50])\nprint(\"Length: \", len(indexed_data))"
    },
    {
      "execution_count": 8,
      "code_cell_id": 7,
      "code": "def index2onehot(batch):\n\n batch_flatten = batch.flatten()\n onehot_flat = np.zeros((batch.shape[0] * batch.shape[1], len(indexer)))\n onehot_flat[range(len(batch_flatten)), batch_flatten] = 1\n onehot = onehot_flat.reshape((batch.shape[0], batch.shape[1], -1))\n\n return onehot"
    },
    {
      "execution_count": 9,
      "code_cell_id": 8,
      "code": "import torch\nfrom torch import nn\nfrom torch import optim\nimport torch.nn.functional as F"
    },
    {
      "execution_count": 10,
      "code_cell_id": 9,
      "code": "class LSTM(nn.Module):\n def __init__(self, char_length, hidden_size, n_layers):\n super().__init__()\n self.hidden_size = hidden_size\n self.n_layers = n_layers\n self.lstm = nn.LSTM(char_length, hidden_size, n_layers, batch_first=True)\n self.output = nn.Linear(hidden_size, char_length)\n\n def forward(self, x, states):\n out, states = self.lstm(x, states)\n out = out.contiguous().view(-1, self.hidden_size)\n out = self.output(out)\n\n return out, states\n\n def init_states(self, batch_size):\n hidden = next(self.parameters()).data.new(self.n_layers, batch_size, self.hidden_size).zero_()\n cell = next(self.parameters()).data.new(self.n_layers, batch_size, self.hidden_size).zero_()\n states = (hidden, cell)\n\n return states"
    },
    {
      "execution_count": 11,
      "code_cell_id": 10,
      "code": "n_seq = 100\nseq_length = 50\nn_batches = math.floor(len(indexed_data) / n_seq / seq_length)\n\ntotal_length = n_seq * seq_length * n_batches\nx = indexed_data[:total_length]\nx = np.array(x).reshape((n_seq,-1))"
    },
    {
      "execution_count": 12,
      "code_cell_id": 11,
      "code": "model = LSTM(len(chars), 256, 2)\nmodel"
    },
    {
      "execution_count": 15,
      "code_cell_id": 12,
      "code": "loss_function = nn.CrossEntropyLoss()\ntorch.autograd.set_detect_anomaly(True)\n\noptimizer = optim.Adam(model.parameters(), lr=0.001)\nepochs = 2"
    }
  ],
  "target": {
    "code_cell_id": 13,
    "code": "losses = []\n\nfor e in range(1, epochs+1):\n states = model.init_states(n_seq)\n batch_loss = []\n\n for b in range(0, x.shape[1], seq_length):\n x_batch = x[:,b:b+seq_length]\n\n if b == x.shape[1] - seq_length:\n y_batch = x[:,b+1:b+seq_length]\n y_batch = np.hstack((y_batch, indexer[\".\"] * np.ones((y_batch.shape[0],1))))\n else:\n y_batch = x[:,b+1:b+seq_length+1]\n\n x_onehot = torch.Tensor(index2onehot(x_batch))\n y = torch.Tensor(y_batch).view(n_seq * seq_length)\n\n pred, states = model(x_onehot, states)\n\n states = tuple(s.detach() for s in states)\n\n loss = loss_function(pred, y.long())\n optimizer.zero_grad()\n loss.backward(retain_graph=True)\n optimizer.step()\n\n batch_loss.append(loss.item())\n\n losses.append(np.mean(batch_loss))\n\n if e%1 == 0:\n print(\"epoch: \", e, \"... Loss function: \", losses[-1])"
  }
}