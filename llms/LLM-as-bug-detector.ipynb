{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d876946c",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 0
   },
   "source": [
    "# LLM-as-bug-detector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00451d45",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 0
   },
   "source": [
    "### Python client\n",
    "\n",
    "A more convenient way of communicating with the server from a notebook, is by using a [python client](https://github.com/ollama/ollama-python) that wraps the REST API.\n",
    "\n",
    "First the python client must be installed (by running the cell below), and then we can proceed to create an instance and use it to request the list of available models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ffbc1455",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "!pip -q install ollama "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a0a392a5-e74f-48f3-aa49-7b03bf24d501",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You are an automated crash detector for machine learning notebooks. Given a sequence of code cells that have been successfully executed, determine whether the next code cell (the target cell) will crash upon execution. Return true it will crash, otherwise return false.\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import config_llm\n",
    "config_llm.prompt_instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2681c4b5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting tensorflow_4: 0th run...\n",
      "Predicting tensorflow_4: 0th run...Attempt 1...\n",
      "Predicting tensorflow_4: 0th run...Attempt 1 succeed.\n",
      "Predicting tensorflow_4: 1th run...\n",
      "Predicting tensorflow_4: 1th run...Attempt 1...\n",
      "Predicting tensorflow_4: 1th run...Attempt 1 succeed.\n",
      "Predicting tensorflow_4: 2th run...\n",
      "Predicting tensorflow_4: 2th run...Attempt 1...\n",
      "Predicting tensorflow_4: 2th run...Attempt 1 succeed.\n",
      "Predicting tensorflow_4: 3th run...\n",
      "Predicting tensorflow_4: 3th run...Attempt 1...\n",
      "Predicting tensorflow_4: 3th run...Attempt 1 succeed.\n",
      "Predicting tensorflow_4: 4th run...\n",
      "Predicting tensorflow_4: 4th run...Attempt 1...\n",
      "Predicting tensorflow_4: 4th run...Attempt 1: LLM call failed with error: POST predict: Post \"http://127.0.0.1:37441/completion\": EOF (status code: 500)\n",
      "Retrying in 10 seconds...\n",
      "Predicting tensorflow_4: 4th run...Attempt 2...\n",
      "Predicting tensorflow_4: 4th run...Attempt 2 succeed.\n",
      "Predictions by mistral-small3.1:latest finished 5 runs, the results are saved in detect_if_cell_crash/mistralsmall31_latest/results_buggy//crash_detection_results_tensorflow_4.json.\n",
      "Predicting tensorflow_11: 0th run...\n",
      "Predicting tensorflow_11: 0th run...Attempt 1...\n",
      "Predicting tensorflow_11: 0th run...Attempt 1 succeed.\n",
      "Predicting tensorflow_11: 1th run...\n",
      "Predicting tensorflow_11: 1th run...Attempt 1...\n"
     ]
    }
   ],
   "source": [
    "# predict if a target cell crash or not\n",
    "import crash_detector\n",
    "import os, json\n",
    "import re\n",
    "\n",
    "is_buggy = True\n",
    "llm_model = \"mistral-small3.1:latest\" #\"llama3:70b\" #\"qwen2.5-coder:latest\"\n",
    "\n",
    "folder_path = f\"detect_if_cell_crash/output_nb_{\"buggy\" if is_buggy else \"fixed\"}_processed\"\n",
    "\n",
    "outputfoldername=f\"detect_if_cell_crash/{re.sub(r'[^\\w]', '', llm_model.replace(':', '_'))}/results_{\"buggy\" if is_buggy else \"fixed\"}/\"\n",
    "id_crash = 0\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith('.json'):\n",
    "        filepath = os.path.join(folder_path, filename)\n",
    "        with open(filepath, 'r', encoding='utf-8') as f:\n",
    "            try:\n",
    "                data = json.load(f)\n",
    "                prompt = crash_detector.format_for_prompt(data)\n",
    "                keyword=os.path.splitext(filename)[0]\n",
    "                if (prompt is not None) and (os.path.exists(f\"{outputfoldername}/crash_detection_results_{keyword}.json\")==False):\n",
    "                    crash_detector.llm_multiple_runs(\n",
    "                        model=llm_model, \n",
    "                        user_message=prompt,\n",
    "                        out_name=keyword,\n",
    "                        outputfolder=outputfoldername,\n",
    "                        runs=5\n",
    "                    )\n",
    "                    id_crash += 1\n",
    "                else:\n",
    "                    print(f\"Skipping {filename}\")\n",
    "            except json.JSONDecodeError:\n",
    "                print(f\"Warning: Failed to parse {filename}\")\n",
    "print(f\"Successfully detected {id_crash} cases\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f05f32-effe-44c3-8652-b4c78c091ef4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# predict if a target cell crash or not\n",
    "import crash_detector\n",
    "import os, json\n",
    "import re\n",
    "\n",
    "is_buggy = False\n",
    "llm_model = \"mistral-small3.1:latest\" # \"llama3:70b\" #\"qwen2.5-coder:latest\"\n",
    "\n",
    "folder_path = f\"detect_if_cell_crash/output_nb_{\"buggy\" if is_buggy else \"fixed\"}_processed\"\n",
    "\n",
    "outputfoldername=f\"detect_if_cell_crash/{re.sub(r'[^\\w]', '', llm_model.replace(':', '_'))}/results_{\"buggy\" if is_buggy else \"fixed\"}/\"\n",
    "id_crash = 0\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith('.json'):\n",
    "        filepath = os.path.join(folder_path, filename)\n",
    "        with open(filepath, 'r', encoding='utf-8') as f:\n",
    "            try:\n",
    "                data = json.load(f)\n",
    "                try:\n",
    "                    prompt = crash_detector.format_for_prompt(data)\n",
    "                except:\n",
    "                    print(filename)\n",
    "                    continue\n",
    "                keyword=os.path.splitext(filename)[0]\n",
    "                if (prompt is not None) and (os.path.exists(f\"{outputfoldername}/crash_detection_results_{keyword}.json\")==False):\n",
    "                    crash_detector.llm_multiple_runs(\n",
    "                        model=llm_model, \n",
    "                        user_message=prompt,\n",
    "                        out_name=keyword,\n",
    "                        outputfolder=outputfoldername,\n",
    "                        runs=5\n",
    "                    )\n",
    "                    id_crash += 1\n",
    "                else:\n",
    "                    print(f\"Skipping {filename}\")\n",
    "            except json.JSONDecodeError:\n",
    "                print(f\"Warning: Failed to parse {filename}\")\n",
    "print(f\"Successfully detected {id_crash} cases\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78eec51e-ca7f-4473-96fb-b463166cb8ef",
   "metadata": {},
   "source": [
    "#### calculate token length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "21aeb5d3-f4d7-42f4-a9cf-c82f8b549dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0697881f-5ac0-4492-b3d1-a97fe0114118",
   "metadata": {},
   "outputs": [],
   "source": [
    "import config_llm\n",
    "import json\n",
    "import crash_detector\n",
    "\n",
    "filepath = \"detect_if_cell_crash/input_nb_buggy_processed/numpy_10.json\"\n",
    "with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "        prompt = crash_detector.format_for_prompt(data)\n",
    "                \n",
    "user_message = prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4f2ac4de-baec-4d13-b630-aafdb60dc546",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2268\n"
     ]
    }
   ],
   "source": [
    "# import tiktoken\n",
    "\n",
    "# encoding = tiktoken.get_encoding(\"cl100k_base\")  # GPT-4-style tokenizer\n",
    "# print(len(encoding.encode(full_input)))  # Approximate token count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b937a257-b916-4a24-8c18-1b1f52eced5b",
   "metadata": {},
   "source": [
    "#### a test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "78a0faca-898c-4c33-a17d-0f7b2bb34557",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You are an automated crash detector for ML notebooks. Given a sequence of code cells that have been successfully executed, determine whether the next code cell (the target cell) will crash upon execution. Output TRUE it will crash, otherwise output FALSE. Do not output anything else.\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config_llm.prompt_instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "82efff26-2c08-44ec-8d56-cfad5eb2f5c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FALSE\n"
     ]
    }
   ],
   "source": [
    "from ollama import Client\n",
    "\n",
    "# Create a client for the LLM-as-a-service\n",
    "client = Client(host='10.129.20.4:9090')\n",
    "\n",
    "response = client.chat(model='mistral-small3.1:latest', \n",
    "                       messages=[\n",
    "                           {'role': 'system', 'content': config_llm.prompt_instruct},\n",
    "                           {'role': 'user', 'content': user_message}])\n",
    "print(response.message.content)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_markers": "\"\"\""
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
